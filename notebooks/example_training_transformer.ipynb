{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438cd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "from openretina.utils.plotting import (\n",
    "    numpy_to_mp4_video,\n",
    ")\n",
    "from openretina.utils.file_utils import get_local_file_path\n",
    "from openretina.utils.h5_handling import load_h5_into_dict\n",
    "from openretina.data_io.cyclers import LongCycler, ShortCycler\n",
    "from openretina.data_io.hoefling_2024.dataloaders import natmov_dataloaders_v2\n",
    "from openretina.data_io.hoefling_2024.responses import filter_responses, make_final_responses\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "import os\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce90df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=os.path.join(\"..\", \"configs\"), version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=\"hoefling_2024_core_readout_low_res.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94c12a",
   "metadata": {},
   "source": [
    "/home/bethge/bkr618/openretina_cache/notebook_example/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4857c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_chosen_root_folder = \"/home/bethge/bkr618/openretina_cache\"  # Change this with your desired path.\n",
    "\n",
    "cfg.paths.cache_dir = your_chosen_root_folder\n",
    "\n",
    "# We will also overwrite the output directory for the logs/model to the local folder.\n",
    "cfg.paths.log_dir = your_chosen_root_folder\n",
    "cfg.paths.output_dir = your_chosen_root_folder\n",
    "\n",
    "os.environ[\"OPENRETINA_CACHE_DIRECTORY\"] = your_chosen_root_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff26970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/bethge/bkr618/openretina_cache/euler_lab/hoefling_2024/stimuli/rgc_natstim_72x64_joint_normalized_2024-10-11.pkl'\n",
    "movie_stimuli = movies_from_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceb8f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc73c344fd3f4f6eba41cf144ce6867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading HDF5 file contents:   0%|          | 0/2077 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset contains 7863 neurons over 67 fields\n",
      " ------------------------------------ \n",
      "Dropped 0 fields that did not contain the target cell types (67 remaining)\n",
      "Overall, dropped 3034 neurons of non-target cell types (-38.59%).\n",
      " ------------------------------------ \n",
      "Dropped 0 fields with quality indices below threshold (67 remaining)\n",
      "Overall, dropped 980 neurons over quality checks (-20.29%).\n",
      " ------------------------------------ \n",
      "Dropped 0 fields with classifier confidences below 0.25\n",
      "Overall, dropped 705 neurons with classifier confidences below 0.25 (-18.32%).\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Final dataset contains 3144 neurons over 67 fields\n",
      "Total number of cells dropped: 4719 (-60.02%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e942ae29934338af5d5a9f90c80582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upsampling natural spikes traces to get final responses.:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses_path = \"/home/bethge/bkr618/openretina_cache/data/euler_lab/hoefling_2024/responses/rgc_natstim_2024-08-14.h5\"\n",
    "responses_dict = load_h5_into_dict(file_path=responses_path)\n",
    "\n",
    "filtered_responses_dict = filter_responses(responses_dict, **cfg.quality_checks)\n",
    "\n",
    "final_responses = make_final_responses(filtered_responses_dict, response_type=\"natural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67502bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5667a2d06404f59997f0e706fce59c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating movie dataloaders:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = natmov_dataloaders_v2(\n",
    "    neuron_data_dictionary=final_responses,\n",
    "    movies_dictionary=movie_stimuli,\n",
    "    allow_over_boundaries=True,\n",
    "    batch_size=128,\n",
    "    train_chunk_size=50,\n",
    "    validation_clip_indices=cfg.dataloader.validation_clip_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29de476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openretina.data_io.base import compute_data_info\n",
    "data_info = compute_data_info(neuron_data_dictionary=final_responses, movies_dictionary=movie_stimuli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efbd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LongCycler(dataloaders[\"train\"])\n",
    "val_loader = ShortCycler(dataloaders[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fe0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/open-retina/openretina/modules/readout/base.py:62: UserWarning: Readout is NOT initialized with mean activity but with 0!\n",
      "  warnings.warn(\"Readout is NOT initialized with mean activity but with 0!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Creating Tokenizer...\n",
      "2. Tokenizer created. Output shape: (43, 56, 64)\n",
      "3. ViViT created with input shape: (43, 56, 64)\n",
      "4. Spatial shape after patching: h=8, w=7\n",
      "5. Core output shape: (64, 43, 8, 7)\n",
      "[SparseAttentionViz] Initialized with outdir=/home/bethge/bkr618/openretina_cache/attn_sparse, n_layers=1, device=cuda, head_limit=None\n"
     ]
    }
   ],
   "source": [
    "n_neurons_dict = data_info[\"n_neurons_dict\"]\n",
    "from openretina.models.core_readout import ViViTCoreReadout\n",
    "\n",
    "model = ViViTCoreReadout(\n",
    "    input_shape=(128,2,50,72,64),\n",
    "    n_neurons_dict=n_neurons_dict,\n",
    "    channels = 2,\n",
    "    Demb=64,  # Embedding dimension\n",
    "    patch_size=12,  # Spatial patch size (H, W)\n",
    "    temporal_patch_size=8,  # Temporal patch size\n",
    "    num_spatial_blocks=3,  # Number of spatial transformer blocks\n",
    "    num_temporal_blocks=2,  # Number of temporal transformer blocks\n",
    "    num_heads=2,  # Number of attention heads\n",
    "    mlp_ratio=4.0,  # MLP expansion ratio\n",
    "    dropout=0.2,\n",
    "    pad_frame=False,\n",
    "    temporal_stride=1,\n",
    "    spatial_stride=8,\n",
    "    ptoken=0.2,  # Token dropout probability\n",
    "    readout_bias=True,\n",
    "    readout_init_mu_range=0.1,\n",
    "    readout_init_sigma_range=0.18,\n",
    "    readout_gamma=0.3,\n",
    "    readout_reg_avg=False,\n",
    "    learning_rate=5e-4,\n",
    "    norm=\"rmsrnorm\",\n",
    "    patch_mode=1,\n",
    "    pos_encoding=4,\n",
    "    reg_tokens=20,\n",
    "    ff_activation = \"gelu\",\n",
    "    drop_path = 0.3,\n",
    "    use_rope = True,\n",
    "    use_causal_attention=False,\n",
    "\n",
    ")\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f81409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Name                                                  | Type                               | Params | Mode \n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "0   | core                                                  | ViViTCoreWrapper                   | 396 K  | train\n",
      "1   | core.tokenizer                                        | Tokenizer                          | 147 K  | train\n",
      "2   | core.tokenizer.proj                                   | Conv3d                             | 147 K  | train\n",
      "3   | core.tokenizer.norm                                   | LayerNorm                          | 128    | train\n",
      "4   | core.tokenizer.spatial_pos_embedding                  | SinusoidalPosEmb                   | 0      | train\n",
      "5   | core.tokenizer.spatial_pos_embedding.dropout          | Dropout                            | 0      | train\n",
      "6   | core.tokenizer.temporal_pos_encoding                  | SinusoidalPosEmb                   | 0      | train\n",
      "7   | core.tokenizer.temporal_pos_encoding.dropout          | Dropout                            | 0      | train\n",
      "8   | core.vivit                                            | ViViT                              | 248 K  | train\n",
      "9   | core.vivit.spatial_transformer                        | Transformer                        | 147 K  | train\n",
      "10  | core.vivit.spatial_transformer.blocks                 | ModuleList                         | 147 K  | train\n",
      "11  | core.vivit.spatial_transformer.blocks.0               | ParallelAttentionBlock             | 49.3 K | train\n",
      "12  | core.vivit.spatial_transformer.blocks.0.norm          | LayerNorm                          | 128    | train\n",
      "13  | core.vivit.spatial_transformer.blocks.0.fused_linear  | Linear                             | 28.7 K | train\n",
      "14  | core.vivit.spatial_transformer.blocks.0.attn_out      | Linear                             | 4.1 K  | train\n",
      "15  | core.vivit.spatial_transformer.blocks.0.ff_out        | Sequential                         | 16.4 K | train\n",
      "16  | core.vivit.spatial_transformer.blocks.0.ff_out.0      | GELU                               | 0      | train\n",
      "17  | core.vivit.spatial_transformer.blocks.0.ff_out.1      | Dropout                            | 0      | train\n",
      "18  | core.vivit.spatial_transformer.blocks.0.ff_out.2      | Linear                             | 16.4 K | train\n",
      "19  | core.vivit.spatial_transformer.blocks.0.drop_path1    | DropPath                           | 0      | train\n",
      "20  | core.vivit.spatial_transformer.blocks.0.drop_path2    | DropPath                           | 0      | train\n",
      "21  | core.vivit.spatial_transformer.blocks.1               | ParallelAttentionBlock             | 49.3 K | train\n",
      "22  | core.vivit.spatial_transformer.blocks.1.norm          | LayerNorm                          | 128    | train\n",
      "23  | core.vivit.spatial_transformer.blocks.1.fused_linear  | Linear                             | 28.7 K | train\n",
      "24  | core.vivit.spatial_transformer.blocks.1.attn_out      | Linear                             | 4.1 K  | train\n",
      "25  | core.vivit.spatial_transformer.blocks.1.ff_out        | Sequential                         | 16.4 K | train\n",
      "26  | core.vivit.spatial_transformer.blocks.1.ff_out.0      | GELU                               | 0      | train\n",
      "27  | core.vivit.spatial_transformer.blocks.1.ff_out.1      | Dropout                            | 0      | train\n",
      "28  | core.vivit.spatial_transformer.blocks.1.ff_out.2      | Linear                             | 16.4 K | train\n",
      "29  | core.vivit.spatial_transformer.blocks.1.drop_path1    | DropPath                           | 0      | train\n",
      "30  | core.vivit.spatial_transformer.blocks.1.drop_path2    | DropPath                           | 0      | train\n",
      "31  | core.vivit.spatial_transformer.blocks.2               | ParallelAttentionBlock             | 49.3 K | train\n",
      "32  | core.vivit.spatial_transformer.blocks.2.norm          | LayerNorm                          | 128    | train\n",
      "33  | core.vivit.spatial_transformer.blocks.2.fused_linear  | Linear                             | 28.7 K | train\n",
      "34  | core.vivit.spatial_transformer.blocks.2.attn_out      | Linear                             | 4.1 K  | train\n",
      "35  | core.vivit.spatial_transformer.blocks.2.ff_out        | Sequential                         | 16.4 K | train\n",
      "36  | core.vivit.spatial_transformer.blocks.2.ff_out.0      | GELU                               | 0      | train\n",
      "37  | core.vivit.spatial_transformer.blocks.2.ff_out.1      | Dropout                            | 0      | train\n",
      "38  | core.vivit.spatial_transformer.blocks.2.ff_out.2      | Linear                             | 16.4 K | train\n",
      "39  | core.vivit.spatial_transformer.blocks.2.drop_path1    | DropPath                           | 0      | train\n",
      "40  | core.vivit.spatial_transformer.blocks.2.drop_path2    | DropPath                           | 0      | train\n",
      "41  | core.vivit.temporal_transformer                       | Transformer                        | 98.6 K | train\n",
      "42  | core.vivit.temporal_transformer.blocks                | ModuleList                         | 98.6 K | train\n",
      "43  | core.vivit.temporal_transformer.blocks.0              | ParallelAttentionBlock             | 49.3 K | train\n",
      "44  | core.vivit.temporal_transformer.blocks.0.norm         | LayerNorm                          | 128    | train\n",
      "45  | core.vivit.temporal_transformer.blocks.0.fused_linear | Linear                             | 28.7 K | train\n",
      "46  | core.vivit.temporal_transformer.blocks.0.attn_out     | Linear                             | 4.1 K  | train\n",
      "47  | core.vivit.temporal_transformer.blocks.0.ff_out       | Sequential                         | 16.4 K | train\n",
      "48  | core.vivit.temporal_transformer.blocks.0.ff_out.0     | GELU                               | 0      | train\n",
      "49  | core.vivit.temporal_transformer.blocks.0.ff_out.1     | Dropout                            | 0      | train\n",
      "50  | core.vivit.temporal_transformer.blocks.0.ff_out.2     | Linear                             | 16.4 K | train\n",
      "51  | core.vivit.temporal_transformer.blocks.0.drop_path1   | DropPath                           | 0      | train\n",
      "52  | core.vivit.temporal_transformer.blocks.0.drop_path2   | DropPath                           | 0      | train\n",
      "53  | core.vivit.temporal_transformer.blocks.1              | ParallelAttentionBlock             | 49.3 K | train\n",
      "54  | core.vivit.temporal_transformer.blocks.1.norm         | LayerNorm                          | 128    | train\n",
      "55  | core.vivit.temporal_transformer.blocks.1.fused_linear | Linear                             | 28.7 K | train\n",
      "56  | core.vivit.temporal_transformer.blocks.1.attn_out     | Linear                             | 4.1 K  | train\n",
      "57  | core.vivit.temporal_transformer.blocks.1.ff_out       | Sequential                         | 16.4 K | train\n",
      "58  | core.vivit.temporal_transformer.blocks.1.ff_out.0     | GELU                               | 0      | train\n",
      "59  | core.vivit.temporal_transformer.blocks.1.ff_out.1     | Dropout                            | 0      | train\n",
      "60  | core.vivit.temporal_transformer.blocks.1.ff_out.2     | Linear                             | 16.4 K | train\n",
      "61  | core.vivit.temporal_transformer.blocks.1.drop_path1   | DropPath                           | 0      | train\n",
      "62  | core.vivit.temporal_transformer.blocks.1.drop_path2   | DropPath                           | 0      | train\n",
      "63  | core.rearrange                                        | Rearrange                          | 0      | train\n",
      "64  | core.activation                                       | ELU                                | 0      | train\n",
      "65  | readout                                               | MultiSampledGaussianReadoutWrapper | 223 K  | train\n",
      "66  | readout.session_1_ventral1_20200226                   | FullGaussian2d                     | 5.7 K  | train\n",
      "67  | readout.session_1_ventral1_20200528                   | FullGaussian2d                     | 3.0 K  | train\n",
      "68  | readout.session_1_ventral1_20200707                   | FullGaussian2d                     | 5.3 K  | train\n",
      "69  | readout.session_1_ventral1_20201021                   | FullGaussian2d                     | 2.3 K  | train\n",
      "70  | readout.session_1_ventral1_20201030                   | FullGaussian2d                     | 2.8 K  | train\n",
      "71  | readout.session_1_ventral1_20210929                   | FullGaussian2d                     | 3.4 K  | train\n",
      "72  | readout.session_1_ventral1_20210930                   | FullGaussian2d                     | 1.8 K  | train\n",
      "73  | readout.session_1_ventral2_20200302                   | FullGaussian2d                     | 2.9 K  | train\n",
      "74  | readout.session_1_ventral2_20200707                   | FullGaussian2d                     | 4.0 K  | train\n",
      "75  | readout.session_1_ventral2_20201021                   | FullGaussian2d                     | 2.8 K  | train\n",
      "76  | readout.session_1_ventral2_20201022                   | FullGaussian2d                     | 3.0 K  | train\n",
      "77  | readout.session_1_ventral2_20201030                   | FullGaussian2d                     | 6.0 K  | train\n",
      "78  | readout.session_1_ventral2_20201117                   | FullGaussian2d                     | 3.3 K  | train\n",
      "79  | readout.session_1_ventral2_20210910                   | FullGaussian2d                     | 3.1 K  | train\n",
      "80  | readout.session_1_ventral2_20210921                   | FullGaussian2d                     | 2.3 K  | train\n",
      "81  | readout.session_1_ventral2_20210929                   | FullGaussian2d                     | 4.2 K  | train\n",
      "82  | readout.session_1_ventral2_20210930                   | FullGaussian2d                     | 4.0 K  | train\n",
      "83  | readout.session_1_ventral2_20211130                   | FullGaussian2d                     | 2.5 K  | train\n",
      "84  | readout.session_2_ventral1_20200226                   | FullGaussian2d                     | 5.1 K  | train\n",
      "85  | readout.session_2_ventral1_20200303                   | FullGaussian2d                     | 3.6 K  | train\n",
      "86  | readout.session_2_ventral1_20200528                   | FullGaussian2d                     | 3.0 K  | train\n",
      "87  | readout.session_2_ventral1_20200529                   | FullGaussian2d                     | 2.1 K  | train\n",
      "88  | readout.session_2_ventral1_20200701                   | FullGaussian2d                     | 3.9 K  | train\n",
      "89  | readout.session_2_ventral1_20201021                   | FullGaussian2d                     | 3.5 K  | train\n",
      "90  | readout.session_2_ventral1_20201030                   | FullGaussian2d                     | 3.9 K  | train\n",
      "91  | readout.session_2_ventral1_20210929                   | FullGaussian2d                     | 3.1 K  | train\n",
      "92  | readout.session_2_ventral2_20200303                   | FullGaussian2d                     | 3.1 K  | train\n",
      "93  | readout.session_2_ventral2_20201016                   | FullGaussian2d                     | 4.2 K  | train\n",
      "94  | readout.session_2_ventral2_20201021                   | FullGaussian2d                     | 3.3 K  | train\n",
      "95  | readout.session_2_ventral2_20201022                   | FullGaussian2d                     | 3.4 K  | train\n",
      "96  | readout.session_2_ventral2_20201030                   | FullGaussian2d                     | 6.5 K  | train\n",
      "97  | readout.session_2_ventral2_20201117                   | FullGaussian2d                     | 3.9 K  | train\n",
      "98  | readout.session_2_ventral2_20210910                   | FullGaussian2d                     | 3.7 K  | train\n",
      "99  | readout.session_2_ventral2_20210921                   | FullGaussian2d                     | 2.7 K  | train\n",
      "100 | readout.session_2_ventral2_20210929                   | FullGaussian2d                     | 3.4 K  | train\n",
      "101 | readout.session_2_ventral2_20210930                   | FullGaussian2d                     | 3.2 K  | train\n",
      "102 | readout.session_2_ventral2_20211130                   | FullGaussian2d                     | 2.2 K  | train\n",
      "103 | readout.session_3_ventral1_20200226                   | FullGaussian2d                     | 5.3 K  | train\n",
      "104 | readout.session_3_ventral1_20200303                   | FullGaussian2d                     | 3.6 K  | train\n",
      "105 | readout.session_3_ventral1_20200529                   | FullGaussian2d                     | 3.1 K  | train\n",
      "106 | readout.session_3_ventral1_20201021                   | FullGaussian2d                     | 2.6 K  | train\n",
      "107 | readout.session_3_ventral1_20201030                   | FullGaussian2d                     | 3.4 K  | train\n",
      "108 | readout.session_3_ventral2_20200302                   | FullGaussian2d                     | 3.1 K  | train\n",
      "109 | readout.session_3_ventral2_20200701                   | FullGaussian2d                     | 3.3 K  | train\n",
      "110 | readout.session_3_ventral2_20200707                   | FullGaussian2d                     | 5.5 K  | train\n",
      "111 | readout.session_3_ventral2_20201022                   | FullGaussian2d                     | 3.6 K  | train\n",
      "112 | readout.session_3_ventral2_20201117                   | FullGaussian2d                     | 3.1 K  | train\n",
      "113 | readout.session_3_ventral2_20210910                   | FullGaussian2d                     | 2.6 K  | train\n",
      "114 | readout.session_3_ventral2_20210921                   | FullGaussian2d                     | 3.6 K  | train\n",
      "115 | readout.session_3_ventral2_20210929                   | FullGaussian2d                     | 2.2 K  | train\n",
      "116 | readout.session_3_ventral2_20210930                   | FullGaussian2d                     | 2.7 K  | train\n",
      "117 | readout.session_3_ventral2_20211130                   | FullGaussian2d                     | 2.6 K  | train\n",
      "118 | readout.session_4_ventral1_20201021                   | FullGaussian2d                     | 2.9 K  | train\n",
      "119 | readout.session_4_ventral1_20201030                   | FullGaussian2d                     | 355    | train\n",
      "120 | readout.session_4_ventral2_20200303                   | FullGaussian2d                     | 2.8 K  | train\n",
      "121 | readout.session_4_ventral2_20200707                   | FullGaussian2d                     | 4.3 K  | train\n",
      "122 | readout.session_4_ventral2_20201022                   | FullGaussian2d                     | 3.8 K  | train\n",
      "123 | readout.session_4_ventral2_20210910                   | FullGaussian2d                     | 2.1 K  | train\n",
      "124 | readout.session_4_ventral2_20210921                   | FullGaussian2d                     | 2.4 K  | train\n",
      "125 | readout.session_4_ventral2_20210929                   | FullGaussian2d                     | 2.9 K  | train\n",
      "126 | readout.session_4_ventral2_20211130                   | FullGaussian2d                     | 2.3 K  | train\n",
      "127 | readout.session_5_ventral1_20200226                   | FullGaussian2d                     | 5.0 K  | train\n",
      "128 | readout.session_5_ventral2_20200303                   | FullGaussian2d                     | 4.2 K  | train\n",
      "129 | readout.session_5_ventral2_20210910                   | FullGaussian2d                     | 2.1 K  | train\n",
      "130 | readout.session_5_ventral2_20210921                   | FullGaussian2d                     | 1.8 K  | train\n",
      "131 | readout.session_5_ventral2_20210929                   | FullGaussian2d                     | 3.1 K  | train\n",
      "132 | readout.session_6_ventral2_20210921                   | FullGaussian2d                     | 2.9 K  | train\n",
      "133 | loss                                                  | PoissonLoss3d                      | 0      | train\n",
      "134 | correlation_loss                                      | CorrelationLoss3d                  | 0      | train\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "619 K     Trainable params\n",
      "0         Non-trainable params\n",
      "619 K     Total params\n",
      "2.479     Total estimated model params size (MB)\n",
      "135       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "\n",
    "\n",
    "summary = summarize(model, max_depth=-1)  # full depth\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8109e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "498c3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_save_path = os.path.join(cfg.paths.output_dir, \"notebook_example\")\n",
    "os.makedirs(log_save_path, exist_ok=True)\n",
    "\n",
    "logger = lightning.pytorch.loggers.TensorBoardLogger(\n",
    "    name=\"tensorboard/\",\n",
    "    save_dir=log_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af37c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = lightning.pytorch.callbacks.EarlyStopping(\n",
    "    monitor=\"val_correlation\",\n",
    "    patience=10,\n",
    "    mode=\"max\",\n",
    "    verbose=False,\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "lr_monitor = lightning.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "model_checkpoint = lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_correlation\", mode=\"max\", save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02d9fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseAttentionViz] Initialized with outdir=/home/bethge/bkr618/openretina_cache/attn_sparse, n_layers=3, device=cuda, head_limit=2\n"
     ]
    }
   ],
   "source": [
    "from openretina.utils.transformer_utils import SparseAttentionViz\n",
    "\n",
    "sparse_cb = SparseAttentionViz(\n",
    "    outdir=\"/home/bethge/bkr618/openretina_cache/attn_sparse\",\n",
    "    n_layers=3,\n",
    "    head_limit=2,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f281dfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = lightning.Trainer(max_epochs=100, logger=logger, callbacks=[early_stopping, lr_monitor, model_checkpoint, sparse_cb], precision = '16-mixed') #add precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60966a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/bethge/bkr618/.local/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name             | Type                               | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | core             | ViViTCoreWrapper                   | 396 K  | train\n",
      "1 | readout          | MultiSampledGaussianReadoutWrapper | 223 K  | train\n",
      "2 | loss             | PoissonLoss3d                      | 0      | train\n",
      "3 | correlation_loss | CorrelationLoss3d                  | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "619 K     Trainable params\n",
      "0         Non-trainable params\n",
      "619 K     Total params\n",
      "2.479     Total estimated model params size (MB)\n",
      "135       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be3c790313f4a079f4e564bd5749aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/open-retina/openretina/modules/readout/gaussian.py:361: UserWarning: the specified feature map dimension is not the readout's expected input dimension\n",
      "  warnings.warn(\"the specified feature map dimension is not the readout's expected input dimension\")\n",
      "/home/bethge/bkr618/open-retina/openretina/models/core_readout.py:96: RuntimeWarning: Regularizer not implemented for ViViTCoreWrapper\n",
      "  regularization_loss_core = self.core.regularizer()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c49716fbb145ec8cf421b62c4da33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/open-retina/openretina/models/core_readout.py:79: RuntimeWarning: Regularizer not implemented for ViViTCoreWrapper\n",
      "  regularization_loss_core = self.core.regularizer()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e135eefd0c2f4a82b9702d4f2e997f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bd93fdb5ff44b7b9e312b7709857fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def5f11409b74ab0ba998e6ff759c651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d68e60d5494eef9e4362f64921bc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fe79dcbb6c46c39ea987377a57c443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f5d8c22483402a8a94041d8c82ad94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8645e62b8ad745589e8a6804670a5443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0dfee495394d7b9072681912dc6189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60be84f752745b990a3099eecdfce72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90aba973dc314dae8c3b37eb4f82d20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f7f44f16f04523bc429830b3c971d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7fa42893474a30939c964dc4989fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd78454160df492d9de8c0fd649e0b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef910f7c754fbbb1b84322d0bae214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a321ad5ca7644af4a52e6daa82345a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998ea19e528346699d88900530792fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0b5d7d12c34c09be5f0bf2fcdf97c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4dc20ee9764d8f80b68f6aa5fa6c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d34c91f15a549c9ac353f403fa290fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb0465cfb254fe9a8596b06b6dda989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5483bb742d3249b79c6245ac405fe11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64af298fcb7b477ba60625c776de00f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832d498c1f7648509f8716f2c32f0d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37a07bd75e7499790a39a43edcf5b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseAttentionViz] Running visualization at end of training (epoch 24)\n",
      "[SparseAttentionViz] Got batch from session: session_1_ventral1_20200226\n",
      "[SparseAttentionViz] Frames shape: torch.Size([15, 2, 150, 72, 64])\n",
      "[SparseAttentionViz] Selected random indices b=14, t=80\n",
      "[SparseAttentionViz] Found core: core\n",
      "[SparseAttentionViz] Created folder: /home/bethge/bkr618/openretina_cache/attn_sparse/epoch024_session_1_ventral1_20200226_b14_t80\n",
      "[SparseAttentionViz] Saved original frame to /home/bethge/bkr618/openretina_cache/attn_sparse/epoch024_session_1_ventral1_20200226_b14_t80/original_frame.png\n",
      "[SparseAttentionViz] Extracting attention from layer -1\n",
      "[SparseAttentionViz] Layer -1 attention shape: torch.Size([2145, 2, 56, 56])\n",
      "[SparseAttentionViz] Extracting attention from layer -2\n",
      "[SparseAttentionViz] Layer -2 attention shape: torch.Size([2145, 2, 56, 56])\n",
      "[SparseAttentionViz] Extracting attention from layer -3\n",
      "[SparseAttentionViz] Layer -3 attention shape: torch.Size([2145, 2, 56, 56])\n",
      "[SparseAttentionViz] Extracted 3 layers with 2 heads each\n",
      "[SparseAttentionViz] Saved 6 individual attention maps\n",
      "[SparseAttentionViz] Saved comprehensive grid to /home/bethge/bkr618/openretina_cache/attn_sparse/epoch024_session_1_ventral1_20200226_b14_t80/all_layers_heads_grid.png\n",
      "[SparseAttentionViz] Visualization complete in folder: /home/bethge/bkr618/openretina_cache/attn_sparse/epoch024_session_1_ventral1_20200226_b14_t80\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "374c6a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   7828 KiB |   8287 MiB |  97261 GiB |  97261 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   7828 KiB |   8287 MiB |  97261 GiB |  97261 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   7637 KiB |   8273 MiB |  97166 GiB |  97166 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  12288 KiB |   9368 MiB |   9368 MiB |   9356 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   4460 KiB |   1375 MiB |  38144 GiB |  38144 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     907    |    1126    |    1890 K  |    1889 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     907    |    1126    |    1890 K  |    1889 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       6    |      67    |      67    |      61    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      42    |      81    |    1070 K  |    1070 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device='cuda', abbreviated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c84b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652217344\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated()/1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43531e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE MODEL ===\n",
      "Allocated: 0.00 GB\n",
      "Reserved: 0.00 GB\n",
      "\n",
      "=== AFTER MODEL TO GPU ===\n",
      "Allocated: 0.00 GB\n",
      "Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear cache first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check before\n",
    "print(\"=== BEFORE MODEL ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to('cuda')\n",
    "\n",
    "print(\"\\n=== AFTER MODEL TO GPU ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22ec6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER CREATING INPUT ===\n",
      "Allocated: 0.13 GB\n",
      "Reserved: 0.14 GB\n"
     ]
    }
   ],
   "source": [
    "# Create dummy batch\n",
    "dummy_input = torch.randn(64, 2, 50, 72, 64).to('cuda')\n",
    "\n",
    "print(\"\\n=== AFTER CREATING INPUT ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91c6e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 73.94 MiB is free. Including non-PyTorch memory, this process has 39.31 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 94.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== AFTER FORWARD PASS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAllocated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.memory_allocated()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/open-retina/openretina/models/core_readout.py:69\u001b[39m, in \u001b[36mBaseCoreReadout.forward\u001b[39m\u001b[34m(self, x, data_key)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch channels t h w\u001b[39m\u001b[33m\"\u001b[39m], data_key: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m     68\u001b[39m     output_core = \u001b[38;5;28mself\u001b[39m.core(x)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     output_readout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreadout\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_core\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output_readout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/open-retina/openretina/modules/readout/multi_readout.py:265\u001b[39m, in \u001b[36mMultiSampledGaussianReadoutWrapper.forward\u001b[39m\u001b[34m(self, data_key, *args, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m readout_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.readout_keys():\n\u001b[32m    264\u001b[39m     out_core = torch.transpose(args[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     out_core = \u001b[43mout_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m[readout_key](out_core, **kwargs)\n\u001b[32m    267\u001b[39m     resp = resp.reshape((args[\u001b[32m0\u001b[39m].size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m, resp.size(-\u001b[32m1\u001b[39m)))\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 73.94 MiB is free. Including non-PyTorch memory, this process has 39.31 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 94.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"\\n=== AFTER FORWARD PASS ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bf1cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model, dummy_input, output  # etc.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80149006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m loss = \u001b[43moutput\u001b[49m.sum()\n\u001b[32m      3\u001b[39m loss.backward()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== AFTER BACKWARD PASS ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== AFTER BACKWARD PASS ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c482d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/134 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|| 134/134 [01:31<00:00,  1.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 67 batches total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = VideoTokenizer(\n",
    "    img_size=(72, 64),\n",
    "    patch_size=(8, 8),\n",
    "    temporal_patch_size=5,\n",
    "    in_channels=2,\n",
    "    Demb=128,\n",
    "    ptoken=0.1\n",
    ").to(device)\n",
    "\n",
    "transformer = SpatioTemporalTransformer(\n",
    "    Demb=128,\n",
    "    num_spatial_blocks=4,\n",
    "    num_temporal_blocks=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    "    chunk_size=64\n",
    ").to(device)\n",
    "\n",
    "tokenizer.eval()\n",
    "transformer.eval()\n",
    "\n",
    "# dictionary for results\n",
    "outputs_dict = {}\n",
    "\n",
    "# loop with progress bar\n",
    "for session_idx, item in enumerate(tqdm(train_loader, desc=\"Processing sessions\", unit=\"session\")):\n",
    "    inputs = item[1].inputs.to(device)\n",
    "    session_name = item[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings, TP, SP = tokenizer(inputs)\n",
    "        output = transformer(embeddings, TP, SP)\n",
    "        output_cpu = output.cpu()\n",
    "\n",
    "    outputs_dict[session_name] = output_cpu\n",
    "\n",
    "    del inputs, embeddings, output\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    torch.cuda.synchronize()  # ensure memory freed before next iteration\n",
    "\n",
    "# summary\n",
    "print(f\"\\nProcessed {len(outputs_dict)} sessions total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb9a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 2, 50, 72, 64])\n",
      "Output shape: torch.Size([128, 128, 10, 9, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the TransformerCoreWrapper\n",
    "core = TransformerCoreWrapper(\n",
    "    input_shape = (128, 2, 50, 72, 64),\n",
    "    in_channels=2,\n",
    "    img_size=(72, 64),\n",
    "    patch_size=(8, 8),\n",
    "    temporal_patch_size=5,\n",
    "    emb_dim=128,\n",
    "    ptoken=0.1,\n",
    "    num_spatial_blocks=4,\n",
    "    num_temporal_blocks=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    "    chunk_size=64,\n",
    "    gamma_weights=0.001,\n",
    "    gamma_attention=0.01,\n",
    ").to(device)\n",
    "\n",
    "# Put in eval mode\n",
    "core.eval()\n",
    "\n",
    "dummy_input = torch.randn(128, 2, 50, 72, 64).to(device)  # (B, C, T, H, W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = core(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "del dummy_input, output\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b97ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:   0%|          | 0/134 [00:00<?, ?session/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  15%|        | 20/134 [00:21<06:12,  3.26s/session]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  29%|       | 39/134 [00:34<01:07,  1.40session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  44%|     | 59/134 [00:59<00:54,  1.38session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  59%|    | 79/134 [01:23<00:41,  1.32session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  74%|  | 99/134 [01:49<00:29,  1.20session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  90%| | 120/134 [02:26<00:50,  3.63s/session]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|| 134/134 [02:39<00:00,  1.19s/session]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 67 sessions total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "viz_folder = Path(\"/home/bethge/bkr618/open-retina/core_visualizations\")\n",
    "os.makedirs(viz_folder, exist_ok=True)\n",
    "\n",
    "core = TransformerCoreWrapper(\n",
    "    input_shape=(128, 2, 50, 72, 64),\n",
    "    in_channels=2,\n",
    "    img_size=(72, 64),\n",
    "    patch_size=(8, 8),\n",
    "    temporal_patch_size=5,\n",
    "    emb_dim=128,\n",
    "    ptoken=0.1,\n",
    "    num_spatial_blocks=4,\n",
    "    num_temporal_blocks=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    "    chunk_size=64,\n",
    "    gamma_weights=0.001,\n",
    "    gamma_attention=0.01,\n",
    ").to(device)\n",
    "\n",
    "core.eval()\n",
    "\n",
    "outputs_dict = {}\n",
    "VISUALIZE_EVERY = 20  # adjust frequency\n",
    "\n",
    "for session_idx, item in enumerate(tqdm(train_loader, desc=\"Processing sessions\", unit=\"session\")):\n",
    "    session_name = item[0]\n",
    "    inputs = item[1].inputs.to(device, non_blocking=True)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            output = core(inputs)\n",
    "            # store CPU copy only\n",
    "            outputs_dict[session_name] = output.detach().cpu()\n",
    "\n",
    "    finally:\n",
    "        # drop GPU refs\n",
    "        del inputs\n",
    "        if \"output\" in locals(): del output\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    if (session_idx + 1) % VISUALIZE_EVERY == 0:\n",
    "        core.save_weight_visualizations(\n",
    "            folder_path=str(viz_folder),\n",
    "            file_format=\"png\",\n",
    "            state_suffix=f\"_session_{session_idx+1}\"\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "print(f\"\\nProcessed {len(outputs_dict)} sessions total.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20048d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_attn_for_batch(core, raw_frames, layer_idx=-1, head=None):\n",
    "    \"\"\"\n",
    "    raw_frames: tensor (B, C, T, H0, W0) in the same format your forward expects.\n",
    "    Returns:\n",
    "        attn: tensor (B, T, num_heads, P, P)\n",
    "    \"\"\"\n",
    "    core.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize exactly how forward does. Your tokenizer likely expects (B,C,T,H,W)\n",
    "        tokens = core.tokenizer(raw_frames)            # (B, T, P, Demb)\n",
    "        attn = core.get_spatial_attention_maps(tokens, layer_idx=layer_idx)  # (B*T, Hn, P, P)\n",
    "        if attn is None:\n",
    "            raise RuntimeError(\"No attention returned. Check layer_idx or get_spatial_attention_maps.\")\n",
    "        B, C, T, H0, W0 = raw_frames.shape\n",
    "        bt, n_heads, P, _ = attn.shape\n",
    "        assert bt == B * T\n",
    "        attn = attn.view(B, T, n_heads, P, P)         # (B, T, Hn, P, P)\n",
    "        if head is not None:\n",
    "            attn = attn[:, :, head:head+1, :, :]      # keep one head\n",
    "        return attn  # (B, T, Hs, P, P)\n",
    "\n",
    "def attn_to_spatial_map(attn, new_h, new_w,\n",
    "                        avg_heads=True,\n",
    "                        agg=\"mean_query\"):\n",
    "    \"\"\"\n",
    "    Convert (B, T, Hs, P, P) --> (B, T, new_h, new_w) spatial importance maps.\n",
    "    agg: \"mean_query\" (average attention over query dimension) or\n",
    "         \"select_query\" (you provide an index) or\n",
    "         \"attn_to_query\" (sum attention of a single query -> shows where that query attends).\n",
    "    By default we average queries to get global key-importance.\n",
    "    \"\"\"\n",
    "    B, T, Hs, P, _ = attn.shape\n",
    "    assert P == new_h * new_w\n",
    "\n",
    "    # Option 1: average heads then average queries -> global importance per key patch\n",
    "    if avg_heads:\n",
    "        attn_h = attn.mean(dim=2)    # (B,T,P,P)\n",
    "    else:\n",
    "        # keep heads dimension\n",
    "        attn_h = attn  # (B,T,Hs,P,P) - not supported by the rest of this simple pipeline\n",
    "        raise NotImplementedError(\"Non-averaged-heads not implemented in this helper.\")\n",
    "\n",
    "    if agg == \"mean_query\":\n",
    "        # for each batch/time, compute importance of each key patch by averaging over queries\n",
    "        # attn_h[b,t,q,k] -> importance_k = mean_q attn_h[b,t,q,k]\n",
    "        importance = attn_h.mean(dim=2)         # (B, T, P)\n",
    "    elif agg == \"sum_query\":\n",
    "        importance = attn_h.sum(dim=2)\n",
    "    else:\n",
    "        raise ValueError(\"agg must be 'mean_query' or 'sum_query'\")\n",
    "\n",
    "    # reshape to patch grid\n",
    "    importance = importance.view(B, T, new_h, new_w)  # (B, T, h, w)\n",
    "\n",
    "    # normalize per map to [0,1]\n",
    "    importance = importance - importance.amin(dim=(2,3), keepdim=True)\n",
    "    denom = importance.amax(dim=(2,3), keepdim=True)\n",
    "    denom[denom == 0] = 1.0\n",
    "    importance = importance / denom\n",
    "\n",
    "    return importance  # (B, T, new_h, new_w)\n",
    "\n",
    "def upsample_to_frame(importance, H0, W0, mode=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    importance: (B, T, h, w) float tensor in [0,1]\n",
    "    returns: (B, T, H0, W0)\n",
    "    \"\"\"\n",
    "    B, T, h, w = importance.shape\n",
    "    x = importance.view(B*T, 1, h, w)\n",
    "    up = F.interpolate(x, size=(H0, W0), mode=mode, align_corners=False)\n",
    "    up = up.view(B, T, H0, W0)\n",
    "    return up\n",
    "\n",
    "def overlay_and_plot(raw_frames, heatmaps, idx_batch=0, idx_time=0, alpha=0.45, cmap='jet'):\n",
    "    \"\"\"\n",
    "    raw_frames: (B, C, T, H0, W0) torch tensor in range [0,1] or similar\n",
    "    heatmaps: (B, T, H0, W0) in [0,1]\n",
    "    idx_batch, idx_time: which frame to show\n",
    "    \"\"\"\n",
    "    img = raw_frames[idx_batch, :, idx_time].cpu().numpy()  # (C, H, W)\n",
    "    heat = heatmaps[idx_batch, idx_time].cpu().numpy()      # (H, W)\n",
    "\n",
    "    # convert C,H,W -> H,W,C for plt\n",
    "    if img.shape[0] == 1:\n",
    "        img_disp = img[0]\n",
    "        plt.imshow(img_disp, cmap='gray', vmin=0, vmax=1)\n",
    "    else:\n",
    "        img_disp = np.transpose(img, (1,2,0))\n",
    "        # if in [0,1] show as is; otherwise normalize\n",
    "        plt.imshow(np.clip(img_disp, 0, 1))\n",
    "\n",
    "    plt.imshow(heat, cmap=cmap, alpha=alpha, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"batch {idx_batch} time {idx_time}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Usage example in one cell ---\n",
    "# raw_frames: (B, C, T, H0, W0) torch.tensor from your dataloader, values scaled to [0,1]\n",
    "# core: instance of ViViTCoreWrapper\n",
    "\n",
    "layer_idx = -1\n",
    "attn = extract_attn_for_batch(core, raw_frames, layer_idx=layer_idx)   # (B, T, Hs, P, P)\n",
    "importance = attn_to_spatial_map(attn, core.new_h, core.new_w, avg_heads=True, agg=\"mean_query\")  # (B,T,h,w)\n",
    "H0, W0 = raw_frames.shape[-2], raw_frames.shape[-1]\n",
    "heat_upsampled = upsample_to_frame(importance, H0, W0)  # (B,T,H0,W0)\n",
    "\n",
    "# plot one example\n",
    "overlay_and_plot(raw_frames, heat_upsampled, idx_batch=0, idx_time=0, alpha=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
