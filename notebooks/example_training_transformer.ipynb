{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438cd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "from openretina.utils.plotting import (\n",
    "    numpy_to_mp4_video,\n",
    ")\n",
    "from openretina.utils.file_utils import get_local_file_path\n",
    "from openretina.utils.h5_handling import load_h5_into_dict\n",
    "from openretina.data_io.cyclers import LongCycler, ShortCycler\n",
    "from openretina.data_io.hoefling_2024.dataloaders import natmov_dataloaders_v2\n",
    "from openretina.data_io.hoefling_2024.responses import filter_responses, make_final_responses\n",
    "from openretina.data_io.hoefling_2024.stimuli import movies_from_pickle\n",
    "import os\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce90df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hydra.initialize(config_path=os.path.join(\"..\", \"configs\"), version_base=\"1.3\"):\n",
    "    cfg = hydra.compose(config_name=\"hoefling_2024_core_readout_low_res.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94c12a",
   "metadata": {},
   "source": [
    "/home/bethge/bkr618/openretina_cache/notebook_example/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4857c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_chosen_root_folder = \"/home/bethge/bkr618/openretina_cache\"  # Change this with your desired path.\n",
    "\n",
    "cfg.paths.cache_dir = your_chosen_root_folder\n",
    "\n",
    "# We will also overwrite the output directory for the logs/model to the local folder.\n",
    "cfg.paths.log_dir = your_chosen_root_folder\n",
    "cfg.paths.output_dir = your_chosen_root_folder\n",
    "\n",
    "os.environ[\"OPENRETINA_CACHE_DIRECTORY\"] = your_chosen_root_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff26970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/bethge/bkr618/openretina_cache/euler_lab/hoefling_2024/stimuli/rgc_natstim_72x64_joint_normalized_2024-10-11.pkl'\n",
    "movie_stimuli = movies_from_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceb8f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973135550afd43d38bc1731065a8d662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading HDF5 file contents:   0%|          | 0/2077 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset contains 7863 neurons over 67 fields\n",
      " ------------------------------------ \n",
      "Dropped 0 fields that did not contain the target cell types (67 remaining)\n",
      "Overall, dropped 3034 neurons of non-target cell types (-38.59%).\n",
      " ------------------------------------ \n",
      "Dropped 0 fields with quality indices below threshold (67 remaining)\n",
      "Overall, dropped 980 neurons over quality checks (-20.29%).\n",
      " ------------------------------------ \n",
      "Dropped 0 fields with classifier confidences below 0.25\n",
      "Overall, dropped 705 neurons with classifier confidences below 0.25 (-18.32%).\n",
      " ------------------------------------ \n",
      " ------------------------------------ \n",
      "Final dataset contains 3144 neurons over 67 fields\n",
      "Total number of cells dropped: 4719 (-60.02%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0161adb7134a01a2be2762934270e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upsampling natural spikes traces to get final responses.:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses_path = \"/home/bethge/bkr618/openretina_cache/data/euler_lab/hoefling_2024/responses/rgc_natstim_2024-08-14.h5\"\n",
    "responses_dict = load_h5_into_dict(file_path=responses_path)\n",
    "\n",
    "filtered_responses_dict = filter_responses(responses_dict, **cfg.quality_checks)\n",
    "\n",
    "final_responses = make_final_responses(filtered_responses_dict, response_type=\"natural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67502bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28308a217f74d8daece92c8b7fe569a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating movie dataloaders:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = natmov_dataloaders_v2(\n",
    "    neuron_data_dictionary=final_responses,\n",
    "    movies_dictionary=movie_stimuli,\n",
    "    allow_over_boundaries=True,\n",
    "    batch_size=128,\n",
    "    train_chunk_size=50,\n",
    "    validation_clip_indices=cfg.dataloader.validation_clip_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29de476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openretina.data_io.base import compute_data_info\n",
    "data_info = compute_data_info(neuron_data_dictionary=final_responses, movies_dictionary=movie_stimuli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efbd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = LongCycler(dataloaders[\"train\"])\n",
    "val_loader = ShortCycler(dataloaders[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fe0d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/open-retina/openretina/modules/readout/base.py:62: UserWarning: Readout is NOT initialized with mean activity but with 0!\n",
      "  warnings.warn(\"Readout is NOT initialized with mean activity but with 0!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Creating Tokenizer...\n",
      "2. Tokenizer created. Output shape: (50, 132, 128)\n",
      "3. ViViT created with input shape: (50, 132, 128)\n",
      "4. Spatial shape after patching: h=12, w=11\n",
      "5. Core output shape: (128, 50, 12, 11)\n"
     ]
    }
   ],
   "source": [
    "n_neurons_dict = data_info[\"n_neurons_dict\"]\n",
    "from openretina.models.core_readout import ViViTCoreReadout\n",
    "\n",
    "model = ViViTCoreReadout(\n",
    "    input_shape=(128,2,50,72,64),\n",
    "    n_neurons_dict=n_neurons_dict,\n",
    "    Demb=128,  # Embedding dimension\n",
    "    patch_size=8,  # Spatial patch size (H, W)\n",
    "    temporal_patch_size=6,  # Temporal patch size\n",
    "    num_spatial_blocks=3,  # Number of spatial transformer blocks\n",
    "    num_temporal_blocks=3,  # Number of temporal transformer blocks\n",
    "    num_heads=4,  # Number of attention heads\n",
    "    mlp_ratio=4.0,  # MLP expansion ratio\n",
    "    dropout=0.1,\n",
    "    pad_frame=True,\n",
    "    temporal_stride=1,\n",
    "    spatial_stride=6,\n",
    "    ptoken=0.1,  # Token dropout probability\n",
    "    readout_bias=True,\n",
    "    readout_init_mu_range=0.05,\n",
    "    readout_init_sigma_range=0.01,\n",
    "    readout_gamma=0.4,\n",
    "    readout_reg_avg=False,\n",
    "    learning_rate=0.001,\n",
    "    norm=\"layernorm\",\n",
    "    patch_mode=1,\n",
    ")\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9856c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Creating Tokenizer...\n",
      "2. Tokenizer created. Output shape: (50, 132, 96)\n",
      "3. ViViT created with input shape: (50, 132, 96)\n",
      "4. Spatial shape after patching: h=12, w=11\n",
      "5. Core output shape: (96, 50, 12, 11)\n"
     ]
    }
   ],
   "source": [
    "n_neurons_dict = data_info[\"n_neurons_dict\"]\n",
    "from openretina.models.core_readout import ViViTCoreReadout\n",
    "model = ViViTCoreReadout(\n",
    "    input_shape=(128, 2, 50, 72, 64),\n",
    "    n_neurons_dict=n_neurons_dict,\n",
    "    Demb=96,                    # ↓ smaller embedding dimension\n",
    "    patch_size=8,\n",
    "    temporal_patch_size=6,\n",
    "    num_spatial_blocks=2,       # ↓ fewer spatial transformer blocks\n",
    "    num_temporal_blocks=2,      # ↓ fewer temporal transformer blocks\n",
    "    num_heads=4,\n",
    "    mlp_ratio=3.0,              # ↓ smaller MLP expansion ratio\n",
    "    dropout=0.3,                # ↑ stronger dropout\n",
    "    pad_frame=True,\n",
    "    temporal_stride=1,\n",
    "    spatial_stride=6,\n",
    "    ptoken=0.2,                 # ↑ stronger token dropout\n",
    "    readout_bias=True,\n",
    "    chunk_size = 64,\n",
    "    readout_init_mu_range=0.05,\n",
    "    readout_init_sigma_range=0.01,\n",
    "    readout_gamma=0.4,\n",
    "    readout_reg_avg=True,       # ↑ regularization on readout weights\n",
    "    learning_rate=5e-4,         # ↓ smaller learning rate for smoother convergence\n",
    "    norm=\"layernorm\",\n",
    "    patch_mode=1,\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f81409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Name                                                  | Type                               | Params | Mode \n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "0   | core                                                  | ViViTCoreWrapper                   | 1.3 M  | train\n",
      "1   | core.tokenizer                                        | Tokenizer                          | 98.6 K | train\n",
      "2   | core.tokenizer.pad                                    | ZeroPad3d                          | 0      | train\n",
      "3   | core.tokenizer.proj                                   | Conv3d                             | 98.3 K | train\n",
      "4   | core.tokenizer.norm                                   | LayerNorm                          | 256    | train\n",
      "5   | core.vivit                                            | ViViT                              | 1.2 M  | train\n",
      "6   | core.vivit.spatial_transformer                        | Transformer                        | 590 K  | train\n",
      "7   | core.vivit.spatial_transformer.blocks                 | ModuleList                         | 590 K  | train\n",
      "8   | core.vivit.spatial_transformer.blocks.0               | ParallelAttentionBlock             | 196 K  | train\n",
      "9   | core.vivit.spatial_transformer.blocks.0.norm          | LayerNorm                          | 256    | train\n",
      "10  | core.vivit.spatial_transformer.blocks.0.fused_linear  | Linear                             | 114 K  | train\n",
      "11  | core.vivit.spatial_transformer.blocks.0.attn_out      | Linear                             | 16.4 K | train\n",
      "12  | core.vivit.spatial_transformer.blocks.0.ff_out        | Sequential                         | 65.5 K | train\n",
      "13  | core.vivit.spatial_transformer.blocks.0.ff_out.0      | GELU                               | 0      | train\n",
      "14  | core.vivit.spatial_transformer.blocks.0.ff_out.1      | Dropout                            | 0      | train\n",
      "15  | core.vivit.spatial_transformer.blocks.0.ff_out.2      | Linear                             | 65.5 K | train\n",
      "16  | core.vivit.spatial_transformer.blocks.0.drop_path1    | Identity                           | 0      | train\n",
      "17  | core.vivit.spatial_transformer.blocks.0.drop_path2    | Identity                           | 0      | train\n",
      "18  | core.vivit.spatial_transformer.blocks.1               | ParallelAttentionBlock             | 196 K  | train\n",
      "19  | core.vivit.spatial_transformer.blocks.1.norm          | LayerNorm                          | 256    | train\n",
      "20  | core.vivit.spatial_transformer.blocks.1.fused_linear  | Linear                             | 114 K  | train\n",
      "21  | core.vivit.spatial_transformer.blocks.1.attn_out      | Linear                             | 16.4 K | train\n",
      "22  | core.vivit.spatial_transformer.blocks.1.ff_out        | Sequential                         | 65.5 K | train\n",
      "23  | core.vivit.spatial_transformer.blocks.1.ff_out.0      | GELU                               | 0      | train\n",
      "24  | core.vivit.spatial_transformer.blocks.1.ff_out.1      | Dropout                            | 0      | train\n",
      "25  | core.vivit.spatial_transformer.blocks.1.ff_out.2      | Linear                             | 65.5 K | train\n",
      "26  | core.vivit.spatial_transformer.blocks.1.drop_path1    | Identity                           | 0      | train\n",
      "27  | core.vivit.spatial_transformer.blocks.1.drop_path2    | Identity                           | 0      | train\n",
      "28  | core.vivit.spatial_transformer.blocks.2               | ParallelAttentionBlock             | 196 K  | train\n",
      "29  | core.vivit.spatial_transformer.blocks.2.norm          | LayerNorm                          | 256    | train\n",
      "30  | core.vivit.spatial_transformer.blocks.2.fused_linear  | Linear                             | 114 K  | train\n",
      "31  | core.vivit.spatial_transformer.blocks.2.attn_out      | Linear                             | 16.4 K | train\n",
      "32  | core.vivit.spatial_transformer.blocks.2.ff_out        | Sequential                         | 65.5 K | train\n",
      "33  | core.vivit.spatial_transformer.blocks.2.ff_out.0      | GELU                               | 0      | train\n",
      "34  | core.vivit.spatial_transformer.blocks.2.ff_out.1      | Dropout                            | 0      | train\n",
      "35  | core.vivit.spatial_transformer.blocks.2.ff_out.2      | Linear                             | 65.5 K | train\n",
      "36  | core.vivit.spatial_transformer.blocks.2.drop_path1    | Identity                           | 0      | train\n",
      "37  | core.vivit.spatial_transformer.blocks.2.drop_path2    | Identity                           | 0      | train\n",
      "38  | core.vivit.temporal_transformer                       | Transformer                        | 590 K  | train\n",
      "39  | core.vivit.temporal_transformer.blocks                | ModuleList                         | 590 K  | train\n",
      "40  | core.vivit.temporal_transformer.blocks.0              | ParallelAttentionBlock             | 196 K  | train\n",
      "41  | core.vivit.temporal_transformer.blocks.0.norm         | LayerNorm                          | 256    | train\n",
      "42  | core.vivit.temporal_transformer.blocks.0.fused_linear | Linear                             | 114 K  | train\n",
      "43  | core.vivit.temporal_transformer.blocks.0.attn_out     | Linear                             | 16.4 K | train\n",
      "44  | core.vivit.temporal_transformer.blocks.0.ff_out       | Sequential                         | 65.5 K | train\n",
      "45  | core.vivit.temporal_transformer.blocks.0.ff_out.0     | GELU                               | 0      | train\n",
      "46  | core.vivit.temporal_transformer.blocks.0.ff_out.1     | Dropout                            | 0      | train\n",
      "47  | core.vivit.temporal_transformer.blocks.0.ff_out.2     | Linear                             | 65.5 K | train\n",
      "48  | core.vivit.temporal_transformer.blocks.0.drop_path1   | Identity                           | 0      | train\n",
      "49  | core.vivit.temporal_transformer.blocks.0.drop_path2   | Identity                           | 0      | train\n",
      "50  | core.vivit.temporal_transformer.blocks.1              | ParallelAttentionBlock             | 196 K  | train\n",
      "51  | core.vivit.temporal_transformer.blocks.1.norm         | LayerNorm                          | 256    | train\n",
      "52  | core.vivit.temporal_transformer.blocks.1.fused_linear | Linear                             | 114 K  | train\n",
      "53  | core.vivit.temporal_transformer.blocks.1.attn_out     | Linear                             | 16.4 K | train\n",
      "54  | core.vivit.temporal_transformer.blocks.1.ff_out       | Sequential                         | 65.5 K | train\n",
      "55  | core.vivit.temporal_transformer.blocks.1.ff_out.0     | GELU                               | 0      | train\n",
      "56  | core.vivit.temporal_transformer.blocks.1.ff_out.1     | Dropout                            | 0      | train\n",
      "57  | core.vivit.temporal_transformer.blocks.1.ff_out.2     | Linear                             | 65.5 K | train\n",
      "58  | core.vivit.temporal_transformer.blocks.1.drop_path1   | Identity                           | 0      | train\n",
      "59  | core.vivit.temporal_transformer.blocks.1.drop_path2   | Identity                           | 0      | train\n",
      "60  | core.vivit.temporal_transformer.blocks.2              | ParallelAttentionBlock             | 196 K  | train\n",
      "61  | core.vivit.temporal_transformer.blocks.2.norm         | LayerNorm                          | 256    | train\n",
      "62  | core.vivit.temporal_transformer.blocks.2.fused_linear | Linear                             | 114 K  | train\n",
      "63  | core.vivit.temporal_transformer.blocks.2.attn_out     | Linear                             | 16.4 K | train\n",
      "64  | core.vivit.temporal_transformer.blocks.2.ff_out       | Sequential                         | 65.5 K | train\n",
      "65  | core.vivit.temporal_transformer.blocks.2.ff_out.0     | GELU                               | 0      | train\n",
      "66  | core.vivit.temporal_transformer.blocks.2.ff_out.1     | Dropout                            | 0      | train\n",
      "67  | core.vivit.temporal_transformer.blocks.2.ff_out.2     | Linear                             | 65.5 K | train\n",
      "68  | core.vivit.temporal_transformer.blocks.2.drop_path1   | Identity                           | 0      | train\n",
      "69  | core.vivit.temporal_transformer.blocks.2.drop_path2   | Identity                           | 0      | train\n",
      "70  | core.rearrange                                        | Rearrange                          | 0      | train\n",
      "71  | core.activation                                       | ELU                                | 0      | train\n",
      "72  | readout                                               | MultiSampledGaussianReadoutWrapper | 424 K  | train\n",
      "73  | readout.session_1_ventral1_20200226                   | FullGaussian2d                     | 10.8 K | train\n",
      "74  | readout.session_1_ventral1_20200528                   | FullGaussian2d                     | 5.7 K  | train\n",
      "75  | readout.session_1_ventral1_20200707                   | FullGaussian2d                     | 10.0 K | train\n",
      "76  | readout.session_1_ventral1_20201021                   | FullGaussian2d                     | 4.3 K  | train\n",
      "77  | readout.session_1_ventral1_20201030                   | FullGaussian2d                     | 5.4 K  | train\n",
      "78  | readout.session_1_ventral1_20210929                   | FullGaussian2d                     | 6.5 K  | train\n",
      "79  | readout.session_1_ventral1_20210930                   | FullGaussian2d                     | 3.5 K  | train\n",
      "80  | readout.session_1_ventral2_20200302                   | FullGaussian2d                     | 5.5 K  | train\n",
      "81  | readout.session_1_ventral2_20200707                   | FullGaussian2d                     | 7.6 K  | train\n",
      "82  | readout.session_1_ventral2_20201021                   | FullGaussian2d                     | 5.3 K  | train\n",
      "83  | readout.session_1_ventral2_20201022                   | FullGaussian2d                     | 5.7 K  | train\n",
      "84  | readout.session_1_ventral2_20201030                   | FullGaussian2d                     | 11.3 K | train\n",
      "85  | readout.session_1_ventral2_20201117                   | FullGaussian2d                     | 6.2 K  | train\n",
      "86  | readout.session_1_ventral2_20210910                   | FullGaussian2d                     | 5.9 K  | train\n",
      "87  | readout.session_1_ventral2_20210921                   | FullGaussian2d                     | 4.5 K  | train\n",
      "88  | readout.session_1_ventral2_20210929                   | FullGaussian2d                     | 8.0 K  | train\n",
      "89  | readout.session_1_ventral2_20210930                   | FullGaussian2d                     | 7.6 K  | train\n",
      "90  | readout.session_1_ventral2_20211130                   | FullGaussian2d                     | 4.7 K  | train\n",
      "91  | readout.session_2_ventral1_20200226                   | FullGaussian2d                     | 9.7 K  | train\n",
      "92  | readout.session_2_ventral1_20200303                   | FullGaussian2d                     | 6.8 K  | train\n",
      "93  | readout.session_2_ventral1_20200528                   | FullGaussian2d                     | 5.7 K  | train\n",
      "94  | readout.session_2_ventral1_20200529                   | FullGaussian2d                     | 3.9 K  | train\n",
      "95  | readout.session_2_ventral1_20200701                   | FullGaussian2d                     | 7.4 K  | train\n",
      "96  | readout.session_2_ventral1_20201021                   | FullGaussian2d                     | 6.6 K  | train\n",
      "97  | readout.session_2_ventral1_20201030                   | FullGaussian2d                     | 7.4 K  | train\n",
      "98  | readout.session_2_ventral1_20210929                   | FullGaussian2d                     | 5.9 K  | train\n",
      "99  | readout.session_2_ventral2_20200303                   | FullGaussian2d                     | 5.9 K  | train\n",
      "100 | readout.session_2_ventral2_20201016                   | FullGaussian2d                     | 8.0 K  | train\n",
      "101 | readout.session_2_ventral2_20201021                   | FullGaussian2d                     | 6.3 K  | train\n",
      "102 | readout.session_2_ventral2_20201022                   | FullGaussian2d                     | 6.5 K  | train\n",
      "103 | readout.session_2_ventral2_20201030                   | FullGaussian2d                     | 12.3 K | train\n",
      "104 | readout.session_2_ventral2_20201117                   | FullGaussian2d                     | 7.4 K  | train\n",
      "105 | readout.session_2_ventral2_20210910                   | FullGaussian2d                     | 7.0 K  | train\n",
      "106 | readout.session_2_ventral2_20210921                   | FullGaussian2d                     | 5.1 K  | train\n",
      "107 | readout.session_2_ventral2_20210929                   | FullGaussian2d                     | 6.5 K  | train\n",
      "108 | readout.session_2_ventral2_20210930                   | FullGaussian2d                     | 6.1 K  | train\n",
      "109 | readout.session_2_ventral2_20211130                   | FullGaussian2d                     | 4.2 K  | train\n",
      "110 | readout.session_3_ventral1_20200226                   | FullGaussian2d                     | 10.1 K | train\n",
      "111 | readout.session_3_ventral1_20200303                   | FullGaussian2d                     | 6.8 K  | train\n",
      "112 | readout.session_3_ventral1_20200529                   | FullGaussian2d                     | 5.8 K  | train\n",
      "113 | readout.session_3_ventral1_20201021                   | FullGaussian2d                     | 5.0 K  | train\n",
      "114 | readout.session_3_ventral1_20201030                   | FullGaussian2d                     | 6.5 K  | train\n",
      "115 | readout.session_3_ventral2_20200302                   | FullGaussian2d                     | 5.8 K  | train\n",
      "116 | readout.session_3_ventral2_20200701                   | FullGaussian2d                     | 6.3 K  | train\n",
      "117 | readout.session_3_ventral2_20200707                   | FullGaussian2d                     | 10.4 K | train\n",
      "118 | readout.session_3_ventral2_20201022                   | FullGaussian2d                     | 6.9 K  | train\n",
      "119 | readout.session_3_ventral2_20201117                   | FullGaussian2d                     | 5.9 K  | train\n",
      "120 | readout.session_3_ventral2_20210910                   | FullGaussian2d                     | 4.9 K  | train\n",
      "121 | readout.session_3_ventral2_20210921                   | FullGaussian2d                     | 6.8 K  | train\n",
      "122 | readout.session_3_ventral2_20210929                   | FullGaussian2d                     | 4.2 K  | train\n",
      "123 | readout.session_3_ventral2_20210930                   | FullGaussian2d                     | 5.1 K  | train\n",
      "124 | readout.session_3_ventral2_20211130                   | FullGaussian2d                     | 4.9 K  | train\n",
      "125 | readout.session_4_ventral1_20201021                   | FullGaussian2d                     | 5.5 K  | train\n",
      "126 | readout.session_4_ventral1_20201030                   | FullGaussian2d                     | 675    | train\n",
      "127 | readout.session_4_ventral2_20200303                   | FullGaussian2d                     | 5.3 K  | train\n",
      "128 | readout.session_4_ventral2_20200707                   | FullGaussian2d                     | 8.1 K  | train\n",
      "129 | readout.session_4_ventral2_20201022                   | FullGaussian2d                     | 7.3 K  | train\n",
      "130 | readout.session_4_ventral2_20210910                   | FullGaussian2d                     | 4.0 K  | train\n",
      "131 | readout.session_4_ventral2_20210921                   | FullGaussian2d                     | 4.6 K  | train\n",
      "132 | readout.session_4_ventral2_20210929                   | FullGaussian2d                     | 5.5 K  | train\n",
      "133 | readout.session_4_ventral2_20211130                   | FullGaussian2d                     | 4.3 K  | train\n",
      "134 | readout.session_5_ventral1_20200226                   | FullGaussian2d                     | 9.6 K  | train\n",
      "135 | readout.session_5_ventral2_20200303                   | FullGaussian2d                     | 8.0 K  | train\n",
      "136 | readout.session_5_ventral2_20210910                   | FullGaussian2d                     | 4.0 K  | train\n",
      "137 | readout.session_5_ventral2_20210921                   | FullGaussian2d                     | 3.5 K  | train\n",
      "138 | readout.session_5_ventral2_20210929                   | FullGaussian2d                     | 5.9 K  | train\n",
      "139 | readout.session_6_ventral2_20210921                   | FullGaussian2d                     | 5.5 K  | train\n",
      "140 | loss                                                  | PoissonLoss3d                      | 0      | train\n",
      "141 | correlation_loss                                      | CorrelationLoss3d                  | 0      | train\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.817     Total estimated model params size (MB)\n",
      "142       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "\n",
    "\n",
    "summary = summarize(model, max_depth=-1)  # full depth\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8109e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "498c3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_save_path = os.path.join(cfg.paths.output_dir, \"notebook_example\")\n",
    "os.makedirs(log_save_path, exist_ok=True)\n",
    "\n",
    "logger = lightning.pytorch.loggers.TensorBoardLogger(\n",
    "    name=\"tensorboard/\",\n",
    "    save_dir=log_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af37c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = lightning.pytorch.callbacks.EarlyStopping(\n",
    "    monitor=\"val_correlation\",\n",
    "    patience=10,\n",
    "    mode=\"max\",\n",
    "    verbose=False,\n",
    "    min_delta=0.001,\n",
    ")\n",
    "\n",
    "lr_monitor = lightning.pytorch.callbacks.LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "model_checkpoint = lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_correlation\", mode=\"max\", save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f281dfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = lightning.Trainer(max_epochs=100, logger=logger, callbacks=[early_stopping, lr_monitor, model_checkpoint], precision = '16-mixed') #add precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60966a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/bethge/bkr618/.local/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name             | Type                               | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | core             | ViViTCoreWrapper                   | 443 K  | train\n",
      "1 | readout          | MultiSampledGaussianReadoutWrapper | 323 K  | train\n",
      "2 | loss             | PoissonLoss3d                      | 0      | train\n",
      "3 | correlation_loss | CorrelationLoss3d                  | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "767 K     Trainable params\n",
      "0         Non-trainable params\n",
      "767 K     Total params\n",
      "3.069     Total estimated model params size (MB)\n",
      "122       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04fd5aa8744eb1b6eed2ea6e8e0287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/open-retina/openretina/modules/readout/gaussian.py:361: UserWarning: the specified feature map dimension is not the readout's expected input dimension\n",
      "  warnings.warn(\"the specified feature map dimension is not the readout's expected input dimension\")\n",
      "/home/bethge/bkr618/open-retina/openretina/models/core_readout.py:93: RuntimeWarning: Regularizer not implemented for ViViTCoreWrapper\n",
      "  regularization_loss_core = self.core.regularizer()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac93f28c54f4f628e5272b40c7065b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/open-retina/openretina/models/core_readout.py:76: RuntimeWarning: Regularizer not implemented for ViViTCoreWrapper\n",
      "  regularization_loss_core = self.core.regularizer()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f1c2c03ffd456baf58119b233ff71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b71e2dab7e44251820ad734e6e15a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdf22590f724499a145118e2124f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a129566f074a84a38adcbea98efd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af1754605774735b0f73439e41e66b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d86dad29de47b8889df3bb866aae0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c870d1005b964060b7e62613e1842b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74900806566946e997769ebd6d491531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139e42deba6440f882542ade0a9bea4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68a1b5223a246ae94b97b2558ea6a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d1434f597e47fa943372e79f8ad9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445f7be33f8340068a0a81bc23da172e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4166483c50bc434da9652db3cd9c6887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cc4158df304c2f92c58b4e3b148520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a020a442854b4a04ae2d76230f5e2dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d32520a6f4201abe01ef270b83a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84b23a8575d47bdae09adee8d5683b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2420c62c124f1dae44309edd766e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae635ae007b34853add421bdd7a477e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b329861e6a4235a4312d57ccd4908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad2d387f2fa473ba02b36182abda574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780603878c7f4e82a7ca6c790b06839c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144e9118b4594530a984424f32ed66b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a9894181504047b8b3fa3bd1ad64b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d231baaf6bf34a10bbbea9ed5ba67c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f69de341ca4474092f4ecee0d06175f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bethge/bkr618/.local/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374c6a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      | 636931 KiB | 636931 KiB | 636931 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         | 636931 KiB | 636931 KiB | 636931 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      | 635907 KiB | 635907 KiB | 635907 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   | 638976 KiB | 638976 KiB | 638976 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   2045 KiB |   2045 KiB |   2045 KiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       2    |       2    |       2    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       2    |       2    |       2    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       2    |       2    |       2    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device='cuda', abbreviated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c84b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652217344\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated()/1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43531e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE MODEL ===\n",
      "Allocated: 0.00 GB\n",
      "Reserved: 0.00 GB\n",
      "\n",
      "=== AFTER MODEL TO GPU ===\n",
      "Allocated: 0.00 GB\n",
      "Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear cache first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check before\n",
    "print(\"=== BEFORE MODEL ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to('cuda')\n",
    "\n",
    "print(\"\\n=== AFTER MODEL TO GPU ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22ec6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER CREATING INPUT ===\n",
      "Allocated: 0.13 GB\n",
      "Reserved: 0.14 GB\n"
     ]
    }
   ],
   "source": [
    "# Create dummy batch\n",
    "dummy_input = torch.randn(64, 2, 50, 72, 64).to('cuda')\n",
    "\n",
    "print(\"\\n=== AFTER CREATING INPUT ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91c6e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 73.94 MiB is free. Including non-PyTorch memory, this process has 39.31 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 94.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== AFTER FORWARD PASS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAllocated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.memory_allocated()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/open-retina/openretina/models/core_readout.py:69\u001b[39m, in \u001b[36mBaseCoreReadout.forward\u001b[39m\u001b[34m(self, x, data_key)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch channels t h w\u001b[39m\u001b[33m\"\u001b[39m], data_key: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m     68\u001b[39m     output_core = \u001b[38;5;28mself\u001b[39m.core(x)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     output_readout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreadout\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_core\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output_readout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/open-retina/openretina/modules/readout/multi_readout.py:265\u001b[39m, in \u001b[36mMultiSampledGaussianReadoutWrapper.forward\u001b[39m\u001b[34m(self, data_key, *args, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m readout_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.readout_keys():\n\u001b[32m    264\u001b[39m     out_core = torch.transpose(args[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     out_core = \u001b[43mout_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m[readout_key](out_core, **kwargs)\n\u001b[32m    267\u001b[39m     resp = resp.reshape((args[\u001b[32m0\u001b[39m].size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m, resp.size(-\u001b[32m1\u001b[39m)))\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 73.94 MiB is free. Including non-PyTorch memory, this process has 39.31 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 94.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"\\n=== AFTER FORWARD PASS ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bf1cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model, dummy_input, output  # etc.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80149006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m loss = \u001b[43moutput\u001b[49m.sum()\n\u001b[32m      3\u001b[39m loss.backward()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== AFTER BACKWARD PASS ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n=== AFTER BACKWARD PASS ===\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c482d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/134 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 134/134 [01:31<00:00,  1.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 67 batches total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = VideoTokenizer(\n",
    "    img_size=(72, 64),\n",
    "    patch_size=(8, 8),\n",
    "    temporal_patch_size=5,\n",
    "    in_channels=2,\n",
    "    Demb=128,\n",
    "    ptoken=0.1\n",
    ").to(device)\n",
    "\n",
    "transformer = SpatioTemporalTransformer(\n",
    "    Demb=128,\n",
    "    num_spatial_blocks=4,\n",
    "    num_temporal_blocks=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    "    chunk_size=64\n",
    ").to(device)\n",
    "\n",
    "tokenizer.eval()\n",
    "transformer.eval()\n",
    "\n",
    "# dictionary for results\n",
    "outputs_dict = {}\n",
    "\n",
    "# loop with progress bar\n",
    "for session_idx, item in enumerate(tqdm(train_loader, desc=\"Processing sessions\", unit=\"session\")):\n",
    "    inputs = item[1].inputs.to(device)\n",
    "    session_name = item[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings, TP, SP = tokenizer(inputs)\n",
    "        output = transformer(embeddings, TP, SP)\n",
    "        output_cpu = output.cpu()\n",
    "\n",
    "    outputs_dict[session_name] = output_cpu\n",
    "\n",
    "    del inputs, embeddings, output\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    torch.cuda.synchronize()  # ensure memory freed before next iteration\n",
    "\n",
    "# summary\n",
    "print(f\"\\nProcessed {len(outputs_dict)} sessions total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb9a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 2, 50, 72, 64])\n",
      "Output shape: torch.Size([128, 128, 10, 9, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the TransformerCoreWrapper\n",
    "core = TransformerCoreWrapper(\n",
    "    input_shape = (128, 2, 50, 72, 64),\n",
    "    in_channels=2,\n",
    "    img_size=(72, 64),\n",
    "    patch_size=(8, 8),\n",
    "    temporal_patch_size=5,\n",
    "    emb_dim=128,\n",
    "    ptoken=0.1,\n",
    "    num_spatial_blocks=4,\n",
    "    num_temporal_blocks=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    "    chunk_size=64,\n",
    "    gamma_weights=0.001,\n",
    "    gamma_attention=0.01,\n",
    ").to(device)\n",
    "\n",
    "# Put in eval mode\n",
    "core.eval()\n",
    "\n",
    "dummy_input = torch.randn(128, 2, 50, 72, 64).to(device)  # (B, C, T, H, W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = core(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "del dummy_input, output\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b97ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:   0%|          | 0/134 [00:00<?, ?session/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  15%|█▍        | 20/134 [00:21<06:12,  3.26s/session]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  29%|██▉       | 39/134 [00:34<01:07,  1.40session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  44%|████▍     | 59/134 [00:59<00:54,  1.38session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  59%|█████▉    | 79/134 [01:23<00:41,  1.32session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  74%|███████▍  | 99/134 [01:49<00:29,  1.20session/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions:  90%|████████▉ | 120/134 [02:26<00:50,  3.63s/session]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer visualizations to /home/bethge/bkr618/open-retina/core_visualizations/transformer_visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|██████████| 134/134 [02:39<00:00,  1.19s/session]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 67 sessions total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "viz_folder = Path(\"/home/bethge/bkr618/open-retina/core_visualizations\")\n",
    "os.makedirs(viz_folder, exist_ok=True)\n",
    "\n",
    "core = TransformerCoreWrapper(\n",
    "    input_shape=(128, 2, 50, 72, 64),\n",
    "    in_channels=2,\n",
    "    img_size=(72, 64),\n",
    "    patch_size=(8, 8),\n",
    "    temporal_patch_size=5,\n",
    "    emb_dim=128,\n",
    "    ptoken=0.1,\n",
    "    num_spatial_blocks=4,\n",
    "    num_temporal_blocks=4,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.1,\n",
    "    chunk_size=64,\n",
    "    gamma_weights=0.001,\n",
    "    gamma_attention=0.01,\n",
    ").to(device)\n",
    "\n",
    "core.eval()\n",
    "\n",
    "outputs_dict = {}\n",
    "VISUALIZE_EVERY = 20  # adjust frequency\n",
    "\n",
    "for session_idx, item in enumerate(tqdm(train_loader, desc=\"Processing sessions\", unit=\"session\")):\n",
    "    session_name = item[0]\n",
    "    inputs = item[1].inputs.to(device, non_blocking=True)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            output = core(inputs)\n",
    "            # store CPU copy only\n",
    "            outputs_dict[session_name] = output.detach().cpu()\n",
    "\n",
    "    finally:\n",
    "        # drop GPU refs\n",
    "        del inputs\n",
    "        if \"output\" in locals(): del output\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    if (session_idx + 1) % VISUALIZE_EVERY == 0:\n",
    "        core.save_weight_visualizations(\n",
    "            folder_path=str(viz_folder),\n",
    "            file_format=\"png\",\n",
    "            state_suffix=f\"_session_{session_idx+1}\"\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "print(f\"\\nProcessed {len(outputs_dict)} sessions total.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
