defaults:
  - data_io: hoefling_2024
  - quality_checks: hoefling_2024
  - dataloader: hoefling_2024
  - model: transformer_core_readoutv2
  - training_callbacks:
    - early_stopping
    - lr_monitor
    - model_checkpoint
  - logger:
    - tensorboard
    - csv
    - mlflow
  - trainer: default_deterministic
  - hydra: default
  - override hydra/sweeper: optuna  # make sure to install `pip install -e ".[optuna]"
  - override hydra/sweeper/sampler: tpe
  - _self_ # values in this config will overwrite the defaults

training_callbacks:
  early_stopping:
    patience: 20

exp_name: transformer_hyperparams_search_3rdtry
seed: 42
check_stimuli_responses_match: false

dataloader:
  batch_size: 128
  train_chunk_size: 50

objective_target: val_correlation

hydra:
  run:
    dir: ${paths.log_dir}/openretina_assets/runs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${paths.log_dir}/openretina_assets/runs/${exp_name}/${now:%Y-%m-%d_%H-%M-%S}
  sweeper:
    sampler:
      seed: 42
    direction: maximize
    study_name: ${exp_name}
    storage: null
    n_trials: 50
    n_jobs: 1 
    params:
      model.patch_size: choice(10, 12)
      model.temporal_patch_size: choice(8, 12)
      model.spatial_stride: choice(8, 10, 12)
      model.num_spatial_blocks: choice(2, 3)
      model.num_temporal_blocks: choice(2, 3)
      model.pad_frame: choice(true, false)
      model.drop_path: interval(0.0, 0.4)
      model.readout_init_mu_range: interval(0.05, 0.2)
      model.readout_init_sigma_range: interval(0.05, 0.2)
      model.readout_gamma: interval(0.1, 0.5)
      trainer.accumulate_grad_batches: choice(1, 5, 10)
      model.learning_rate: interval(1e-4, 1e-3)
    
model:
  channels: 2
  patch_size: 8
  temporal_patch_size: 6
  spatial_stride: 6
  temporal_stride: 1
  Demb: 64
  ptoken: 0.3
  pad_frame: true
  norm: "layernorm" 
  patch_mode: true
  pos_encoding: 4
  num_heads: 2
  reg_tokens: 10
  num_spatial_blocks: 2
  num_temporal_blocks: 2
  dropout: 0.2
  mlp_ratio: 4.0
  ff_activation: "layernorm"  #transformer core feedforward activation function
  drop_path: 0.4  #stochastic depth rate
  use_rope: true  #use rotary positional embeddings
  use_causal_attention: false  #whether to use causal attention in temporal blocks
  readout_bias: true
  readout_init_mu_range: 0.1
  readout_init_sigma_range: 0.15
  readout_gamma: 0.4
  readout_reg_avg: false
  learning_rate: 5e-4



paths: #change the output path
  cache_dir: ${oc.env:OPENRETINA_CACHE_DIRECTORY} # Remote files are downloaded to this location
  # If data_dir is a local path, data will be read from there. If a remote link, the target will be downloaded to cache_dir.
  data_dir: ${paths.cache_dir}/data/ # Choose the location of the data. Should be used in data_io functions.
  log_dir: "/mnt/lustre/work/bethge/bkr618/hyperparams_search" # Used as parent for output_dir. Will store train logs.
  output_dir: ${hydra:runtime.output_dir} # Modify in the "hydra/default.yaml" config
  movies_path: '/home/bethge/bkr618/openretina_cache/euler_lab/hoefling_2024/stimuli/rgc_natstim_72x64_joint_normalized_2024-10-11.pkl'
  responses_path:  "/home/bethge/bkr618/openretina_cache/data/euler_lab/hoefling_2024/responses/rgc_natstim_2024-08-14.h5"
# Overwrite model defaults with specifics for the current data input format

trainer:
  max_epochs: 100
  precision: 16-mixed
  accumulate_grad_batches: 1

